{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports, huggingface login, cuda checking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gsapountzakis\\working_dir\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import math\n",
    "from huggingface_hub import login\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from bitsandbytes.optim import Adam8bit\n",
    "import os\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "import re\n",
    "from pypdf import PdfReader\n",
    "import json\n",
    "from toolz import compose\n",
    "\n",
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1+cu124\n",
      "True\n",
      "1\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Tokenizer and handle document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_page_lines(text):\n",
    "    return re.sub(\n",
    "        r'\\d+\\s*Biotech\\s*\\(\\d{4}\\)\\s*\\d+:\\d+\\s*\\d+\\s*\\d+\\s*Page\\s*\\d+\\s*of\\s*\\d+.*',\n",
    "        \"\",\n",
    "        text,\n",
    "        flags=re.IGNORECASE,\n",
    "    )\n",
    "\n",
    "def remove_citations(text): #(numbers)\n",
    "    return re.sub(\n",
    "        r'\\([^\\)]*\\d+[^\\)]*\\)',\n",
    "        \"\",\n",
    "        text,\n",
    "        flags=re.IGNORECASE,\n",
    "    )\n",
    "\n",
    "def remove_declarations_and_references(text): \n",
    "    return re.sub(\n",
    "        r'Declarations[^\\n]*\\n(?:[^\\n]*\\n)+',\n",
    "        \"\",\n",
    "        text,\n",
    "        flags=re.IGNORECASE,\n",
    "    )\n",
    "\n",
    "def remove_intro(text):\n",
    "    return re.sub(\n",
    "        r'[^\\n]*\\n(?:[^\\n]*\\n)+Abstract',\n",
    "        \"\",\n",
    "        text,\n",
    "        flags=re.IGNORECASE,\n",
    "    )\n",
    "\n",
    "def remove_keywords(text):\n",
    "    return re.sub(\n",
    "        r'Keywords[^\\n]*\\n(?:[^\\n]*\\n)',\n",
    "        \"\",\n",
    "        text,\n",
    "        flags=re.IGNORECASE,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text_pipeline = compose(\n",
    "    remove_citations,\n",
    "    remove_page_lines,\n",
    "    remove_declarations_and_references,\n",
    "    remove_intro,\n",
    "    remove_keywords\n",
    ")\n",
    "\n",
    "reader = PdfReader(\"Deep_learning_tools_for_advancing_drug_discovery.pdf\")\n",
    "text = \"\"\n",
    "for page in reader.pages[1:19]:  \n",
    "    page_text = page.extract_text()\n",
    "    cleaned_page_text = clean_text_pipeline(page_text)\n",
    "    text += cleaned_page_text + \"\\n\\n\"  # na to ksanado edo\n",
    "\n",
    "text = re.sub(r'\\.\\n', '.\\n\\n\\n', text)\n",
    "\n",
    "with open(\"processed_output.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed_output_manual3.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "dataset_entries = [\n",
    "    {\"text\": paragraph} for paragraph in text.split(\"\\n\\n\") if paragraph.strip()\n",
    "]\n",
    "with open(\"dataset3.jsonl\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for entry in dataset_entries:\n",
    "        file.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 219 examples [00:00, 2605.97 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 219\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hf_dataset = Dataset.from_json(\"dataset3.jsonl\")\n",
    "\n",
    "print(hf_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Tokenizer\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.model_max_length = 128\n",
    "print(\"Loaded Tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 219/219 [00:00<00:00, 3221.91 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 175\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 44\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(data):\n",
    "    return tokenizer(data[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_data = hf_dataset.map(tokenize_function, batched=True, batch_size=32,num_proc=1,)\n",
    "tokenized_data = tokenized_data.train_test_split(test_size=0.2)\n",
    "print(tokenized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model and LoRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n",
      "Loaded Model\n"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\")#,quantization_config=bnb_config)\n",
    "print(model)\n",
    "print(\"Loaded Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Freeze all parameters\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "\n",
    "# for param in model.lm_head.parameters():#model.model.layers[12:15].parameters():\n",
    "#     param.requires_grad = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate total and trainable parameters\n",
    "# total_params = sum(p.numel() for p in model.parameters())\n",
    "# trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# print(total_params)\n",
    "# print(trainable_params)\n",
    "\n",
    "# # Calculate the total size in bytes\n",
    "# def model_size_in_gb(model):\n",
    "#     total_size = 0\n",
    "#     for param in model.parameters():\n",
    "#         total_size += param.numel() * param.element_size()\n",
    "#     for buffer in model.buffers():\n",
    "#         total_size += buffer.numel() * buffer.element_size()\n",
    "#     return total_size / (1024 ** 3)  # Convert bytes to gigabytes\n",
    "\n",
    "# # Get model size on GPU\n",
    "# gpu_size = model_size_in_gb(model)\n",
    "# print(f\"Model size on GPU: {gpu_size:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If targeting all linear layers\n",
    "target_modules = [\n",
    "    \"q_proj\",\n",
    "    \"k_proj\",\n",
    "    \"v_proj\",\n",
    "    \"o_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"down_proj\",\n",
    "    \"up_proj\",\n",
    "    \"lm_head\",\n",
    "]\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # 8 16\n",
    "    target_modules=target_modules,\n",
    "    lora_alpha=8,  # 32 8\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "\n",
    "lora_model.print_trainable_parameters()\n",
    "print(lora_model)\n",
    "print(\"Loaded Model with LoRA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Unfreeze the last N layers (e.g., last 2 transformer blocks)\n",
    "# for name, param in lora_model.named_parameters():\n",
    "#     if  name.split('.')[4]==('15' or '13' or '12' or '11' or '10')  and name.split('.')[5]==('self_attn' or 'mlp') :\n",
    "#         param.requires_grad = True\n",
    "#     print(name,param.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model = lora_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train save and evaluate model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01,  # 0.01\n",
    "    push_to_hub=False,\n",
    "    output_dir=\"./results_lora_10\",\n",
    "    num_train_epochs=30,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    warmup_steps=10,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=1000,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=1,\n",
    "    # deepspeed=\"./ds_config1.json\",\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=2,\n",
    "    load_best_model_at_end = True\n",
    "    #optim=\"paged_adamw_8bit\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")  # ,pad_to_multiple_of=4\n",
    "trainer = Trainer(\n",
    "    model=lora_model, #model\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training started\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1496' max='2610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1496/2610 7:44:35 < 5:46:25, 0.05 it/s, Epoch 17/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.736198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.486528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.354820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.270620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.211726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.167627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.135380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.101021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.073367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.038384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.003973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>6.286200</td>\n",
       "      <td>2.982990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>6.286200</td>\n",
       "      <td>2.968334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>6.286200</td>\n",
       "      <td>2.962331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>6.286200</td>\n",
       "      <td>2.966345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>6.286200</td>\n",
       "      <td>2.965339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>6.286200</td>\n",
       "      <td>2.967022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gsapountzakis\\working_dir\\.venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "c:\\Users\\gsapountzakis\\working_dir\\.venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "c:\\Users\\gsapountzakis\\working_dir\\.venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "c:\\Users\\gsapountzakis\\working_dir\\.venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "c:\\Users\\gsapountzakis\\working_dir\\.venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "c:\\Users\\gsapountzakis\\working_dir\\.venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "c:\\Users\\gsapountzakis\\working_dir\\.venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "c:\\Users\\gsapountzakis\\working_dir\\.venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "c:\\Users\\gsapountzakis\\working_dir\\.venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "c:\\Users\\gsapountzakis\\working_dir\\.venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "c:\\Users\\gsapountzakis\\working_dir\\.venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "c:\\Users\\gsapountzakis\\working_dir\\.venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "c:\\Users\\gsapountzakis\\working_dir\\.venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "c:\\Users\\gsapountzakis\\working_dir\\.venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "c:\\Users\\gsapountzakis\\working_dir\\.venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "c:\\Users\\gsapountzakis\\working_dir\\.venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "c:\\Users\\gsapountzakis\\working_dir\\.venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training finished\n"
     ]
    }
   ],
   "source": [
    "print(\"training started\")\n",
    "torch.cuda.empty_cache()\n",
    "trainer.train()\n",
    "print(\"training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gsapountzakis\\working_dir\\.venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved trainer locally\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"./fine_tuned_model_lora_10\")\n",
    "print(\"saved trainer locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44' max='44' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [44/44 03:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.9623308181762695, 'eval_runtime': 193.2627, 'eval_samples_per_second': 0.228, 'eval_steps_per_second': 0.228, 'epoch': 17.0}\n",
      "evaluation finished\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)\n",
    "print(\"evaluation finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lora = aplo\n",
    "#lora 2 = lora\n",
    "#lora 3 = unfreeze 12:15\n",
    "#lora 4 = qlora\n",
    "#lora 5 = many epochs lora\n",
    "#lora 6 = many epochs lora + unfreeze\n",
    "#lora 7 =  many epochs lora + unfreeze self_attn,mlp 10-15\n",
    "#lora 8 =  many epochs lora + unfreeze self_attn,mlp 10-15 modified dataset\n",
    "#lora 9 =  many epochs lora + unfreeze self_attn,mlp 10-15 modified dataset even more\n",
    "#lora 10 =  many epochs lora + modified dataset even more\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
