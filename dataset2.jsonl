{"text": "\nA few decades ago, drug discovery and development were limited to a bunch of medicinal chemists working in a lab with \nenormous amount of testing, validations, and synthetic procedures, all contributing to considerable investments in time and \nwealth to get one drug out into the clinics. The advancements in computational techniques combined with a boom in multi-\nomics data led to the development of various bioinformatics/pharmacoinformatics/cheminformatics tools that have helped \nspeed up the drug development process. But with the advent of artificial intelligence (AI), machine learning (ML) and deep \nlearning (DL), the conventional drug discovery process has been further rationalized."}
{"text": "\nExtensive biological data in the form \nof big data present in various databases across the globe acts as the raw materials for the ML/DL-based approaches and helps \nin accurate identifications of patterns and models which can be used to identify therapeutically active molecules with much \nfewer investments on time, workforce and wealth. In this review, we have begun by introducing the general concepts in the \ndrug discovery pipeline, followed by an outline of the fields in the drug discovery process where ML/DL can be utilized. "}
{"text": "\nWe have also introduced ML and DL along with their applications, various learning methods, and training models used to \ndevelop the ML/DL-based algorithms. Furthermore, we have summarized various DL-based tools existing in the public \ndomain with their application in the drug discovery paradigm which includes DL tools for identification of drug targets and \ndrug\u2013target interaction such as DeepCPI, DeepDTA, WideDTA, PADME DeepAffinity, and DeepPocket. "}
{"text": "\nAdditionally, we have discussed various DL-based models used in protein structure prediction, de novo design of new chemical scaffolds, \nvirtual screening of chemical libraries for hit identification, absorption, distribution, metabolism, excretion, and toxicity \n(ADMET) prediction, metabolite prediction, clinical trial design, and oral bioavailability prediction. In the end, we have \ntried to shed light on some of the successful ML/DL-based models used in the drug discovery and development pipeline \nwhile also discussing the current challenges and prospects of the application of DL tools in drug discovery and development. \nWe believe that this review will be useful for medicinal and computational chemists searching for DL tools for use in their \ndrug discovery projects."}
{"text": "\nIntroduction\nThe term artificial intelligence (AI), commonly referred to \nas the intelligence demonstrated by machines, is used to \nindicate instances in which a system/machine show cog-\nnitive abilities like humans, such as learning and problem \nsolving, have been considered to be a game-changer across \nall industries, both academic and commercial . The World Economic Forum stated that the \namalgamation of big data and AI would kick start the fourth \nindustrial revolution that can radically alter the practice of \nscientific discovery . Like any other sector, \nthe pharmaceutical sector is considering the unrealized \nprospects of AI to address key problems influencing drug \ndiscovery and productivity . "}
{"text": "\nIn the pharmaceutical industry, AI started gaining popularity \nwhen AI-based models demonstrated biological/chemical \nproperty predictions with great accuracy in a short time, \nespecially DL architectures . Moreover, \nboth advancements in hardware and the ease of availability \nof large datasets have contributed to the tremendous growth \nin the application of AI in pharmaceutical research ."}
{"text": "\nThe majority of the problems arising during the process \nof drug discovery include unfavorable absorption, distribu-\ntion, metabolism, excretion, and toxicity (ADMET) proper-\nties, which have been known to be a major cause of failure \nof the potential molecules in the drug development pipe-\nline, contributing to large consumption of time, capital and \nhuman resources . This has increased the \ninterest in the early-stage prediction of ADMET proper -\nties of drug candidates so that the success rate of a com-\npound reaching the later stages of drug development can \nbe enhanced . "}
{"text": "\nAI has been effectively utilized to develop models and prediction tools for ADMET \nproperties. Apart from property predictions, AI has also \ncontributed to early phases of drug discovery like de novo \ndesigning of chemical compounds and peptides . Moreover, companies involved in \nclinical research have ascertained that revising the research \nstrategies by introducing AI-based techniques has resulted \nin greater success rates in both preclinical and clinical trials . "}
{"text": "\nWorth mentioning is the contribution of AI to precision \nmedicine, which has been helping researchers maximize \npatients\u2019 benefit and lower the side effects caused due to \nthe prevalent traditional \u201cone size fits all\u201d approaches. The \nnew revolutionizing AI methodologies have been helping in \ncharacterizing different subgroups of diseases, patient strati-\nfication, and studying the underlying factor unique to the \nspecific form of the disease ."}
{"text": "\nThis review mainly focusses on the DL-based tools which \nare used in different stages of the drug discovery pipeline. \nWe have given a general introduction to drug discovery pro-\ncess, Machine learning and DL techniques, followed by spe-\ncific examples and discussion on various DL and AI-based \ntools for drug development. We also pointed out some nota-\nble success stories in the use of AI and DL in drug develop-\nment and medicine."}
{"text": "\nIntroduction to\u00a0drug discovery and\u00a0development\nDrug discovery and development is a complex process that \naims to identify and develop novel therapeutics against \nvalidated biological target intrinsically associated with a \nparticular disease of interest . An \noverview of drug discovery and development is shown in \nFig.\u00a01. It encompasses the whole process of bringing a drug \nmolecule into the market, initially starting with hit identi-\nfication and ending with clinical trials phases after being \napproved by regulating bodies such as the Food and Drug \nAdministration FDA . "}
{"text": "\nTraditionally, the drug discovery process starts with identification of bimo-\nlecular targets in the body which have been validated to \nplay a vital role in the disease pathology. This is followed \nby high-throughput screening experiments in which large \nchemical libraries are screened against the selected target \nusing appropriate assay . The main objective of this high-throughput screening is to find prom-\nising compounds which have the potential to be developed \ninto a drug candidate to treat the disease under consideration ."}
{"text": "\nAfter high-throughput screening, the \nchosen compounds, often referred to as hit compounds, are \nanalyzed for their biological activity via in\u00a0vitro studies. \nThe most potent compounds obtained from in\u00a0vitro activ -\nity data are developed into lead compounds through lead \noptimization process. During the lead optimization phase, \nthe compounds are modified to improve its bioavailability, \nsolubility, partition coefficient, and stability as these factors \ncan have direct impact of the drug therapeutic efficacy and \npotency. The molecules with optimized ADMET properties \nare then further evaluated for their effectiveness using suit-\nable animal models . "}
{"text": "\nThe optimized compounds are tested in human subjects in order to validate \nand confirm the potency, therapeutic efficacy, ADMET and \npossible adverse drug reactions through a four step process \ncalled clinical trials, in which each step is carried out in \nvarying number of human subjects in a randomized control \nmanner . If the drug candidate yields desired \nresults in the clinical trial phase, then it is approved by the \nregulating bodies like FDA, after which the drug is released \ninto the market ."}
{"text": "\nIntroduction to\u00a0machine learning and\u00a0deep learning\nML is a method of data analysis involving development of \nnew algorithms and models capable of interpreting multi-\ntude of data . While considering ML, one must not confuse it with AI, as accord-\ning to the FDA \u201call ML techniques are AI techniques, but not \nall AI techniques are ML techniques\u201d. Furthermore, FDA \nalso defines AI as \u201cthe science and engineering of making \nintelligent machines\u201d, and ML as \u201can AI technique that can \nbe used to design and train software algorithms to learn from \nand act on data\u201d ."}
{"text": "\nThe algorithms used in recent years have successively \nimproved their performance with the increase in both the \nquantitative and qualitative aspects of data available for \nlearning . ML is considered as one of the best options available when applied \nto solve problems for which a big amount of data and vari-\nous variables are available to the individual but a model or \nformula relating these various variables amongst themselves \nalong with the expected result is not known . However, \nwhen drug discovery moved into an era of a large amount \nof data, ML approaches evolved into DL approaches, which \nare a more powerful and efficient to deal with the massive \namounts of data generated from modern drug discovery \napproaches . "}
{"text": "\nML allows a computer system to make some \npredictions or decisions, based on its past experience, with-\nout being explicitly programmed. In contrast to the tradi-\ntional physical models that rely on particular physical equa-\ntions, the ML technique uses several algorithms to create a \npattern, leading to the prediction of chemical, biological, and \nphysical properties of the novel compounds. This is mainly \ndone using two learning techniques, supervised learning \nand unsupervised learning techniques. Supervised ML is \nthe construction of an algorithm capable of generating pat-\nterns and hypotheses to predict the fate of future instances, \ndepending on the data provided . "}
{"text": "\nIn brief, supervised learning technique uses the input data to \ntrain the algorithm and create a decision boundary to clas-\nsify or predict the outcomes in any similar circumstances. \nThe supervised ML algorithms can be further classified into \nclassification algorithm and regression algorithm ."}
{"text": "\nThe classification algorithm aims at categorizing the data \nbased on the training dataset. One of the common appli-\ncations of classification algorithm in bioinformatics is the \nidentification of gene coding regions in a Genome. These \ntools are considered to be significantly flawless as several \nclassification algorithms are used to train from a given set \nof datasets and are further used to classify the gene coding \nregions in a genome . "}
{"text": "\nThe regression algorithm aims at predicting the fate of future instances \ndepending on the data in the training dataset. It uses vari-\nous techniques such as rule-based techniques, logic-based \ntechniques, instance-based techniques, stochastic techniques, \netc. to make accurate predictions. Recently, the regression \nalgorithms are being widely used to predict the novel targets \nor structures such as the protein\u2013protein interaction sites. A \ndetailed study about regression algorithms have witnessed \nsome promising results with an accuracy of above 80% to \nidentify the structures in proteomics ."}
{"text": "\nOne of the biggest advantages of the supervised learning \nalgorithms is that it can be trained specifically by setting \nan ideal decision boundary, the user gets the authority to \ndetermine the number of classes, and the input data are well-\nlabeled which makes the output of the test algorithm to be \nmore accurate and reliable. While on the other hand, some of \nthe disadvantages of supervised learning techniques include \nthe classification of huge datasets, which can be sometimes \nbe challenging and time consuming, overtraining of decision \nboundaries due to the unavailability of appropriate exam-\nples, which can make the output of the test algorithm to be \ninaccurate. Moreover, data preparation and pre-processing \nof the input data can also be a challenging task ."}
{"text": "\nThe unsupervised ML aims at interpretation and learning \nan abstract representation of the given data in the absence of \nany predefined labels, or phenotypes. Unsupervised learning \nworks by clustering data points into patterns to obtain mean-\ningful biological information. Unsupervised learning can \nbe broadly classified into clustering, which relies on the \nprinciple of the grouping of unlabeled data based on their \nsimilarities and association, which involves discovering \nsome relationships between the attributes of unlabeled data \npoints. "}
{"text": "\nSome of the commonly used clustering algorithms include hierarchical or k-means clustering . The clustering technique splits the unlabeled dataset \ninto groups based on their similarities. For a huge amount \nof data, k-means clustering is commonly used to form clus-\nters of small molecule profiles, on the basis of their profile \nsimilarity . The k-means clustering and \nthe corresponding heat maps are comparatively simple and \nrequire little computational resources. The hierarchical clus-\ntering is often used in gene expression profiling or genetic \ninteraction studies which provides a broad visualization of \nthe data, hence helping in better analysis ."}
{"text": "\nOne of the major advantages of unsupervised learn-\ning techniques over supervised learning algorithms is that \nit is less complex as compared to supervised learning as \ntraining of the dataset is not required and sorting of raw \ndata and understanding the different models of learning \nmakes it useful in real time. Also, it is much easier to get \nunlabeled data from a computer automatically rather than \nlabeled data which needs human involvement . "}
{"text": "\nSome of the major disadvantages of unsu-\npervised learning techniques includes data sorting which \nmay not be precise as the data used is not labeled which \nmay lead to less accurate and unpredictable results. Due \nto the absence of any prior knowledge or training set of \ndata, the spectral classes do not always correspond to the \ninformational classes, hence spectral properties of classes \nmay change with time which makes the class information \nto vary while moving from one image to another . All these learning techniques mentioned above have \ncontributed a lot in making the slow, tedious and expen-\nsive process of drug discovery to be efficient, fast and cost \neffective."}
{"text": "\nDL is a subset of ML based on artificial neural networks \nthat use multiple layers to progressively extract higher \nlevel features from raw input. Due to its ability to learn \nfrom data and the environment, DL and neural network \n(NN), also known as artificial neural networks (ANN) \nnamed after its artificial representation of the working of \na human nervous system, have become one of the most \nsuccessful techniques in various AI research areas . "}
{"text": "\nIt has shown superior perfor -\nmance over other ML methods and has recently emerged \nas one of the most promising tools in the field of phar -\nmaceutical research where its application is not only lim-\nited to the bioactivity predictions but has far exceeded in \naddressing numerous problems in the field of drug design \nand discovery . Table\u00a0 1 enlists some of \nthe extensively used tools in the field of DL ."}
{"text": "\nFigure\u00a02 represents a typical NN with the working of a \nsingle neuron. Artificial neural network is a structure which \nhas got several input vectors I  = [i1, i2, i3, \u2026, in] and an \nappropriate output vector O = [o1, o2, o3,\u2026, om] along with \nseveral connected elementary units, known as neurons. The \nnetwork initially receives information in the form of input \nvectors and aims to process or learn from it. From here, the \ndata go through one or more hidden layers which understand \nthe hidden patterns in the data using calculations and carry \nout transformations accordingly. The activation function or \nthe transfer function also acts upon the processed data to \ncapture the non-linear relationship between the inputs and \nalso convert it to a more usable output . The \nperformance of an ANN depends on the number of layers, \nthe number of neurons, transfer function, the presence of \na bias, and the way neurons are interconnected ."}
{"text": "\nDeep learning tools for\u00a0drug discovery\nTo realize the enormous potential of DL algorithms in drug \ndiscovery and development, computer scientist and medici-\nnal chemist have found a common point to work together to \ndevelop DL-based tools, predictive models and algorithms \nwhich can be used in the drug discovery and development. \nHere, we have summarized some of the DL-based tools \nwhich are developed to aid the drug discovery and develop-\nment process."}
{"text": "\nDeep learning tools for\u00a0predicting drug\u2013target \ninteractions and\u00a0binding affinity\nDrug\u2013target interaction refers to the interactions between \nchemical compounds and bimolecular drug targets in the \nhuman body and it plays an extremely important role in \ndrug discovery and development as the therapeutic effect \nis a result of this interaction. The very small and limited \nknowledge about drug\u2013target interactions based on wet-lab \nexperiments have caused a huge gap between the known and \nunknown drug\u2013target pairs which has increased the interest \nin the search for efficient methods of drug\u2013target interac-\ntions (DTI) prediction. "}
{"text": "\nThe traditional practices for DTI prediction have been facing monetary and technical limita-\ntions, while computational strategies have been proved to \nshow efficiency in doing the same. At present, the major \ncomputational approaches used in DTI prediction includes \nligand-based approach, docking simulation, chemo-genomic \napproach, text mining methods, ML/ DL based methods, \nand network-based methods, a branch diagram about vari-\nous tools and approaches is given in Fig.\u00a03 . Some of the DL-based methods for \ndrug\u2013target interaction are discussed below."}
{"text": "\nThe computational prediction of drug target in reaction \ncan be largely classified into DTI prediction methods and \ndrug target binding affinity (DTBA) prediction methods. \nThe DTI prediction methods contains docking simulations, \ngene ontology-based methods, ligand-based methods, text \nmining-based methods and ML/DL/Network-based methods. \nDTBA-based methods are further classified into structure \nbased and non-structure based whereas structure based are \nfurther classified into classical scoring function methods and \nML-SF/DL-SF methods and classical scoring function meth-\nods are further classified into knowledge based, empirical SF \nbased and Force-field SF-based methods."}
{"text": "\nDTI\u2011CNN\nThe convolutional neural network (CNN) is a class of \nNN, commonly used to analyze visual imagery. DTI-CNN is a simple DL-based drug\u2013target interaction prediction tool that \nis said to outperform the existing state-of-the-art methods \nby the intelligent interaction of three components namely, \n heterogeneous-network-based feature extractor,  denoising-auto encoder-based feature selector,  CNN-\nbased interaction predictor . As the model is based on random walk with restart \n(RWR) and denoising auto encoder (DAE) model, it is \ncapable of coping up with low-dimensional feature vec-\ntors and noisy incomplete and high-dimensional features \nfrom heterogeneous data sources, including drug, pro-\nteins, side-effects and diseases information. The general \nworkflow of DTI-CNN-based DTI prediction is shown \nin Fig.\u00a0 4."}
{"text": "\nDTI\u2011CNN\nThe first step in using DTI-CNN for DTI prediction \nincludes the construction of a heterogeneous network by \nthe integration of a large number of drugs and protein \nrelated information sources and using the RWR model to \nobtain the initial drug feature vector and protein feature \nvector. In the next step with the use of the DAE model, \nthe low-dimensional representations of the high-dimen-\nsional features of drugs and proteins are obtained. In the \nfinal step, under the known drug\u2013protein interactions, \nthe samples are divided into positive samples and nega-\ntive samples. The CNN model is then used to predict the \nassociation between each pair of drugs and proteins using \nthe feature vector of drug\u2013protein pairs . Another \nsuch DL-based tool called FRnet-DTI, which is a CNN-\nbased classifier for drug target interaction prediction and \nuses an auto-encoder-based feature manipulation ."}
{"text": "\nDeepCPI\nDeepCPI is a tool based on novel framework that uses DL \ntechniques along with unsupervised representation learning \nmeant for prediction of DTI . At first, it is \nsaid to use latent semantic analysis and Word2vec methods \nto learn the low-dimensional feature representations of both \nthe compounds and proteins in an unsupervised manner. \nThen the feature embedding of the compounds and protein \nfrom the first step is fed to the DL network for successful \nprediction of the drug\u2013protein interaction. "}
{"text": "\nDeepCPI\nThe development of efficient DTI predicting models is still under progress due \nto the large number of limitations it faces. For example, in \nthe case of docking simulation, the 3D structure of the target \nprotein is very much required, but is not always available. \nMoreover, it is evident that in comparison to the DTI, the \nstrength of the binding (binding affinity) between a drug and \nits target is much more informative for drug development \nprocess which helps to gain deeper insights into the intermo-\nlecular forces operating between them ."}
{"text": "\nDeepCPI\nThe newly developed DL algorithms to predict DTBA \nhave shown superior performance in comparison to conven-\ntional ML algorithms. These DL-based algorithms use sim-\nplified molecular input line entry system (SMILES) which \nis a compact text format of representation of the molecu-\nlar structure in which each chemical entity is mapped to a \nsingle ASCII strings of 20\u201390 characters, ligand maximum \ncommon substructure (LMCS), extended connectivity fin-\ngerprint (ECFP) or a mixture of the three as an input data \nor as drug features. They also use different neural network \n(NN) types that have their own unique strengths which allow \nthem to be suitable for a varied range of applications ."}
{"text": "\nDeepDTA\nThe first DTBA predicting DL-based approach is DeepDTA \n which is a non-structure-based method \nand uses SMILES for input data for drugs. The amino acids \nand the protein sequences are similarly encoded in SMILES. \nThe CNN used in DeepDTA has three 1D convolutional lay-\ners which follow max-pooling functions which are referred \nto as the first CNN block and are applied on the drug embed-\nding to learn latent features. For the protein embedding, a \nsimilar CNN block is constructed and applied to it. "}
{"text": "\nDeepDTA\nDeep-DTA is said to tune a large number of hyper-parameters \nincluding number of filters, filter length of drug and protein, \nbatch size, optimizer, and learning rate in the validation step. \nThis model aims to minimize the difference in the values \nof the predicted and real DBTA in the training period. The \nlimitations faced in this model due to the usage of CNN can \nbe overcome by using much more appropriate architectures, \nwhich can learn from long protein sequences, like the long-\nshort term memory ."}
{"text": "\nWideDTA\nWideDTA is a CNN DL model that uses four text-based \nsources of information as input which include,  ligand \nSMILES (LS),  protein sequences (PS),  ligand maxi-\nmum common substructure (LMCS),  protein domains \nand motifs (PDM). WideDTA differs from DeepDTA as it \nrepresents LS and PS as a set of words instead of full-length \nsequences. In PS, a word is three-residues in the sequences, \nwhile in LS it is an eight-residue. The makers claim that the \nfeatures of the protein that are represented by shorter lengths \nof residues are not detected in full-length sequences due to \nthe low signal-to-noise ratio and hence the WideDTA is a \nword-based model instead of a character-based one ."}
{"text": "\nPADME\nThe DL-based DTBA predicting method which uses \ndrug\u2013target features and fingerprints to different DNN, is \ncalled protein and drug molecule interaction prediction \n(PADME). When extended-connectivity fingerprint is \nused as the input for the representation of drugs, the tool is \nknown as PADME-ECFP. The other PADME version inte-\ngrates molecular graph convolution into the model to learn \nthe latent features of drugs from SMILES by adding one \nmore graph convolution neural network and is represented \nas PADME-GraphConv. "}
{"text": "\nPADME\nProtein sequence composition descriptors are used by both versions which contain rich \ninformation for representing target proteins. Once the feature \nvectors are generated, it is fed into the simple feedforward \nneural network for a particular drug\u2013target pair for predict-\ning the DTBA ."}
{"text": "\nDeepAffinity\nThe DL DTBA predicting model which only relies on \nSMILES for representing drugs is known as DeepAffin-\nity. To represent the proteins, DeepAffinity relies on the \nstructural property sequence representation that annotates \nthe sequence with structural information, which are shorter \nthan the other representations, provides structural details \nefficiently and gives higher resolution of the sequences, \nalso the regression task is highly benefited. Using a famous \nRNN model, seq2seq, the representations are encoded into \nan embedding form. "}
{"text": "\nDeepAffinity\nAt later stages of data processing, the \nRNN encoders are coupled with a CNN model whose output \nrepresentations for the drug and target are concatenated and \nto get the final DTBA output values are fed into the FC lay-\ners. The complete model which includes data representation, \nembedding learning, and joint supervised learning trained \nfrom end to end. Basically, the RNN-CNN pipeline has been \nfound to produce high accuracy results when compared to \nother ML-based methods used on the same dataset ."}
{"text": "\nDeep docking\nVirtual prediction of protein ligand interaction by the help of \ndocking can tremendously reduce the time required for the \nprocess of new drug discovery and development , but the speed of virtual \nscreening-based DTI prediction is offered a limitation due to \nthe large chemical library with billions of compounds. Thus, \na much faster screening technique is required to tackle and \nfilter such huge amount of data, deep docking is a new and \nnovel method that works on the principles of DL platform \nwhich is capable of docking billions of molecular structures \nin a quick and precise manner . "}
{"text": "\nDeep docking\nThe main objective of deep docking is to reduce the database \nof billions of compounds into a few millions of subset of \ndata while retaining the extensive majority of the possible \nvirtual hits which can then be carried forward for the actual \ndocking studies or can be further processed or refined with \nthe combination of other methods of virtual screening to \nfurther narrow down the data containing more concentrated \nvirtual hits. Deep docking relies on the deep neural network \n(DNN) training where the training set is expanded along \nwith the predicted hit molecules form each previous steps of \ninterpretation gradually making more rigorous cutoff toward \nthe end of the calculation. "}
{"text": "\nDeep docking\nIt employs the use of quantitative \nstructure activity relationship (QSAR) models trained on the \nbasis of docking scores obtained for the subsets of a chemi-\ncal library to predict the docking results for yet to be docked \ncompounds and thus eliminate the molecules not showing \npromise to yield a good docking score in a constant manner. \nIt utilizes the use of quick computing and large independent \nQSAR descriptors such as 2D molecular fingerprints , as a result of using DL to quickly determine the \ndocking scores for a large dataset of compounds for which \nactual docking is yet to be performed. Deep Docking can \nattain up to 100 folds improvement in virtual screening \nspeed and about 6000 folds improvement for identification of \ntop ranked compounds thereby avoiding the loss of favorable \nvirtual hits ."}
{"text": "\nDeepBAR\nDeepBAR is a DL-based binding affinity predicting tool \ndeveloped by the scientists at MIT by combining chemistry \nand ML . The binding free energy \nmeasures the affinity between a drug molecule and its target. \nTo obtain the best drug amongst a given number of potential \nones, the drug with the lowest binding free energy value \nshould be chosen as it will be able to disrupt the protein\u2019s \nnormal function effectively."}
{"text": "\nDeepBAR\nTherefore, it can be said that fast and accurate calcula-\ntion of standard binding free energy has many important \napplications in this drug discovery process. The BAR in \nDeepBAR refers to the Bennett acceptance ratio method. \nBAR is an outdated algorithm that is used to calculate bind-\ning free power. DeepBAR utilizes the Bennet acceptance \nratio along with the information from different endpoints \nand other intermediate circumstances."}
{"text": "\nProtein structure prediction\nProteins are the one of the most important biomolecules \nwhich plays a wide variety of roles in an organism, like \nenzymatic activity, receptor activity, cell signaling, hormo-\nnal activities, intracellular transport, etc. Most of the cur -\nrently identified drug targets are also protein molecules, \nwhose malfunction leads to pathological states. To study \nthe function of proteins, it is important to know the structure \nof the proteins, as the protein structure usually determines \nits function, activity and also pathological conditions. But \nthe process of identifying the structure is not very straight \nforward and it requires experimental procedures like X-ray \ncrystallography, NMR spectroscopy, cryo electron micros-\ncopy, etc., which are time consuming and in most cases, \nvery difficult to perform. "}
{"text": "\nThe traditional method of experimental data threw light on the structures of around 100,000 \nunique proteins, which is just a fraction of the known pro-\ntein sequences. To address the gap of such a large number \nof unknown protein structures, accurate computational \napproaches are needed, which enables large-scale structural \nbioinformatics . To make protein struc-\nture determination, a much simpler and less tedious proce-\ndure, scientist took help of various DL techniques which can \npredict protein structure with great degree of confidence, \nsome of which are discussed below."}
{"text": "\nAlphafold\nAlphaFold  is a new computational method that \nincludes analysis of the covariation in the homologous \nsequences, which are in contact and helps in the prediction \nof protein structures. The initial step of this modern method \nincludes training a neural network to make precise predic-\ntions of the distances between residue pairs, inferring a bet-\nter idea about the structure. This information can be used to \nconstruct a potential of mean force that can precisely mark \nout the shape of the protein. Considering the chances of \nhaving sequences with fewer homologous sequences, the \npotential of mean force can be optimized by a simple gradi-\nent descent algorithm, named AlphaFold, which helps in \nachieving higher accuracy without complex sampling pro-\ncedures ."}
{"text": "\nAlphafold\nAlphaFold generally assembles the best probable frag-\nments based on the analysis of the multiple sequence align-\nment. It interprets the spatial proximity by detecting the \nmutations that have occurred throughout the evolutionary \ntimeframes in response to the other mutations. For such a \ntask, it utilizes a huge amount of computational power in \norder to manage the truly DNN that identify the evolutionary \npatterns within the protein structure sequences with respect \nto the contact distributions and angular restraints. Addition-\nally, with the help of DL algorithms it can produce a protein \nspecific statistical potential using a \u2018learned reference state\u2019, \ninstead of a physical-based reference state. Thus, AlphaFold \nhas given the researchers a tool that has great potential in \nprotein structure prediction ."}
{"text": "\nCASP\nThe objective of critical assessment of protein structure \nprediction (CASP)  is to develop a technology that \ncan recognize and construct the three dimensional struc-\nture of the protein from the protein sequences. It can be \nachieved primarily by two ways on the basis of the pres-\nence of any template structure previously available or not, \nnamely  template-based models and  template-free \nmodels, amongst which the template-based modeling is \nthe more preferable one if a good template is available, \nas it utilizes the available protein structure as the base for \nprediction, thereby more matured technique among the two \nand can be readily approached by the researchers who have \nless experience. On the other hand, if there is no template \npresent for the protein structure one can employ template-\nfree modeling to build the structure. "}
{"text": "\nCASP\nTemplate-free modeling has two kinds of approaches, fragment-based assembly and \nde novo folding, where the de novo folding approach aims \nto build the three dimensional structures from the scratch \nusing the fundamentals of physics, where the secret to its \nsuccess lies within the use of an accurate energy function to \nefficiently search for the lowest energy state conformation \nand also to discriminate native like structures from decoys. \nYet, the fragment-based assembly still dominates because of \nits accuracy and better capability in protein structure predic-\ntion when there is no good template available."}
{"text": "\nDL tools for\u00a0compound de novo design\nDe novo design is defined as a process to generate novel \nmolecules according to DTBA / DTI data or pharmacoph-\nore data, where a pharmacophore is the minimum struc -\ntural requirements needed for showing biological activity \n. The de novo design aims at \ndiscovering novel drug-like compounds. In contrast to the \nearly software tools, which have a tendency to discover new \ncompounds of some limited chemical attraction and diver -\nsity, the modern de novo design algorithms put focus on the \nsynthesizability and drug-likeness properties of the com-\npounds . "}
{"text": "\nThe previous de novo algorithms utilized structure-based approaches to grow \nligands that will be able to fit the binding site of the target of \ninterest both sterically and electronically. One of the major \ndrawbacks of these approaches is that the molecules created \noften possess poor drug metabolism and pharmacokinetic \nproperties and are synthetically intractable . Apart from the structure-based approach, the ligand-\nbased approaches were also widely used. Although these \nmethods are said to generate novel structures effectively \nbased on the transformation or reaction rules, the inherent \nrigidness and scope of the synthesizability pose restrictions. "}
{"text": "\nThe approach usually involves the generation of a large vir-\ntual library of compounds, usually a chemical space which \nis searched using a function that takes several parameters \nlike the drug metabolism and pharmacokinetics profiles \ninto account. These virtual libraries are created either using \nchemical reactions along with a group of available chemical \nbuilding blocks or using transformational rules based on the \nexpertise of medicinal chemists ."}
{"text": "\nNow with the big data revolution, and the development \nof DL algorithms, de novo methodologies based on deep \nreinforcement learning (RL) have come into action which \nhelps in generating compounds with the desired physical, \nchemical, and bioactivity properties . These deep reinforcement algorithms \ninvolve the analysis of possible actions and estimation of \nthe statistical relationship between the actions and their pos-\nsible outcomes, which is followed by the determination of \na treatment system that attempts to find the most desirable \noutcome. Let us look at some DL models for the de novo \ndesign."}
{"text": "\nReLeaSE\nOne of the deep RL approaches used here which enables the \ndesign of chemical libraries with desired properties is rein-\nforcement learning for structural evolution (ReLeaSE). The \nmost distinct aspect of this approach is the use of SMILES \nto represent the molecules . Figure\u00a0 5 \nshows the classical representation of reinforcement learning \nfor structural evolution."}
{"text": "\nReLeaSE\nThe ReLeaSE method includes two DNN, namely the \ngenerative and the predictive which are trained in two stages. \nIn stage I, the models are trained separately using different \nalgorithms, while in stage II, the models are trained together \nusing the RL approach. The generative model in this system \nproduces chemically feasible novel molecules, while the pre-\ndictive model, estimates the generative model\u2019s conduct by \nallocating a numerical reward or penalty to every molecule \nthat has been generated. The generative model is trained to \nmaximize the expected reward."}
{"text": "\nGenerative artificial intelligence (AI)\u2011based model\nThis model is based on generative AI which claims that it \nautonomously designs novel chemical compounds using the \nknowledge of known bioactive compounds along with inher-\nited bioactivity and synthesizability . A general overview about generative model is given \nin Fig.\u00a06\nThis approach consists of two steps, one of which \nincludes the creation of a generic model which has learned \nthe constitution of drug like molecules from a set of large \nunfocussed compounds. In the second step, the already exist-\ning model is tuned on more specific molecular aspects from \na small target-focused library of actives. A deep recurrent \nneural network is utilized for training the generic model \n."}
{"text": "\nDeepScaffold\nDrug design aims to find novel compounds which have desir-\nable pharmacological features. One of the efficient methods \nto develop potential drug candidates is by retaining certain \nscaffolds as the core structures.\nDeepScaffold is a scaffold-based molecular generative \nmodel which conducts molecule generation based on scaf-\nfold definitions for drug discovery. Scaffold here refers to the \ncore structure of a molecule. These include Bemis\u2013Murcko \nscaffolds, cyclic skeletons, and scaffolds with specifications \non side-chain properties. "}
{"text": "\nDeepScaffold\nDeepScaffold is capable of generalizing the learned chemical rules of the addition of atoms \nand bonds to a given scaffold. The molecules generated by \nthis method were evaluated by molecular docking in the D2 \ndopamine receptor  targets, and the results obtained \ncan then be successfully applied to solve a large number \nof drug design problems including the generation of com-\npounds containing a given scaffold and de novo drug design \nof potential drug candidates with specific docking scores. \nAn overview of how DeepScaffold performs the function is \nshown in Fig.\u00a07."}
{"text": "\nAIScaffold\nAnother such AI-based tool is AIScaffold (https:// iaidr  ug. \nstone wise. cn), a web-based tool which is mainly utilized for \nscaffold diversification with the use of a deep generative \nmodel . Scaffold diversification is often used \nby medicinal chemists for the purpose of lead compound \noptimization, but the software tools for the same are not \nreadily available. AIScaffold is one such tool that can be \nutilized for scaffold diversification, unlike other tools which \nare designed to develop results by utilizing the information \nin molecular scaffolds. An overview of AIScaffold function-\ning is shown in Fig.\u00a08"}
{"text": "\nAIScaffold\nThis tool is capable of performing large-scale diversifica-\ntion that is up to 500,000 molecules within a matter of sev -\neral minutes and then as a result recommend the top 500 or \nthe top 0.1% molecules. It also provides additional features \nsuch as site-specific diversification. By facilitating the scaf-\nfold diversification process, AIScaffold is helping medicinal \nchemists in accelerating drug design."}
{"text": "\nDESMILES\nDESMILES is based on the principles of DNN model which \nis a ML approach toward a newer drug design and works on \nthe basis of recurrent neural network (RNN) architecture \n. DESMILES mainly aims to pro -\nduce a series of small molecules which could be chemically \nidentical to the given input of small molecule, it utilizes \nmolecular fingerprint as an input and then corresponds it \nto a matching sequence of SMILES string with an objec-\ntive of correlating the fingerprint to the SMILES string. "}
{"text": "\nDESMILES\nDESMILES could be incorporated in the early phase of \ndrug design and discovery process where it could be used in \ncombination with various molecular screening technologies \nin order to identify some newer scaffolds for possible drug \ncandidates. Extended connectivity fingerprint is another \nform of representations that are explicitly designed to cap-\nture various molecular features that may be relevant to the \nmolecular activity, and is thus advantageous over SMILES \nform of representation but lacks the structural details of the \nmolecule thereby making it difficult to generate molecular \nstructures from the fingerprints."}
{"text": " \nDL tools for\u00a0hit identification using virtual screening\nVirtual screening is another in silico technique used in drug \ndiscovery to search large libraries of small molecules to \nidentify those hits which have a higher probability to bind to \na drug target. The virtual screening technique uses biologi-\ncal, topological, and physicochemical properties of a chemi-\ncal compound as well as the targets. The virtual screening \nmethods can be mainly divided into two categories: (I) \nstructure-based, which uses the 3D structure of targets and \nchemical compounds to model and visualize the interactions . "}
{"text": "\nThe 3D structure \ncan be obtained by X-ray crystallography or by Nuclear \nmagnetic resonance. Once the 3D structural information \nis collected, docking can be applied to find the interaction \nbetween a compound and a particular target. (II) Non-struc-\nture based which can be further subdivided into two groups, \n(a) ligand-based virtual screening  which employs the molecular properties \nof compounds to model and analyzes the interactions with \ntargets and (b) proteo-chemometric modeling which includes \ncombining non-structural descriptors and targets at the input \nlevel ."}
{"text": "\nMany studies have shown that DL algorithms show much \nbetter results in comparison with other ML algorithms in the \ncase of virtual screening, especially due to their impactful \napplications in the de novo molecular design, where mol -\necules with desired properties are generated by the utiliza-\ntion of sequence data . Some of the DL-based tools used in Virtual screen-\ning and QSAR are discussed below."}
{"text": "\nDeep VS\nDeepVS is a DL approach to improve docking-based virtual \nscreening. In DeepVS, the output from the docking program \nis used for the extraction of relevant features from the availa-\nble basic data from protein ligand complexes. This approach \nuses atom and amino acid embeddings for implementing an \neffective way of creating distributed vector representations \nof protein\u2013ligand complexes by modeling the compound as \na set of atom contexts that is further processed by a convo-\nlutional layer. It also has an added advantage of having no \nrequirement of feature engineering . The \nworkflow of Deep VS is represented in the Fig.\u00a09."}
{"text": "\nSIEVE score\nSIEVE score is a newly developed AI-based virtual screen-\ning tool, also called Similarity of Interaction Energy Vec-\ntor Score . \nThis Virtual screening tool has been said to be a promising \nmethod for hit identification and aims to enrich potentially \nactive compounds from a large library of chemical com-\npounds for the purpose of biological experiments. This tool \nis said to have a better performance in terms of efficiency as \ncompared to the state-of-the-art virtual screening methods \nwhich were based on ML. The screening results obtained \nfrom this tool are also human interpretable in the form of \nimportant interactions which make it easy to distinguish \nbetween active and inactive compounds ."}
{"text": "\nSimilarity search\nSimilarity searching is a method that allows fast identifica-\ntion of chemical analogs to biologically active compounds \n. It is a deep-learning model which follows the prin-\nciples of ligand-based drug design, where it utilizes the \nmolecular signatures as vectors to construct a connected \nneural network. A promising technique to predict various \nproperties/parameters that are essential for hit identification \nlike the bioactivity, aqueous solubility and toxicity based \non the structure of the compound under investigation. Such \nmulti-task neural network models can help in predicting \nthe activity for multiple targets for the same hit much more \neffectively as compared to the single task networks because \nof the better representation of the data and much accurate \nrecognition of the general patterns within the data."}
{"text": "\nSimilarity search\nSimilarity search is a two-step de novo approach which is \nbased on PERL script, where specific inputs are utilized to \nimprove the efficiency of the resulting outcome. This tech-\nnique combines the DNN along with the ML approach as \na scoring function for the virtual screening or for predict-\ning the affinity of the compound under investigation for the \nselected target. By this method, one can test numerous com-\npounds against a single target or to test a single target with a \nsingle compound . The model was applied \nto a single drug, namely piperine, and its experimental tar -\ngets. A general docking approach and molecular dynamics \nsimulation approaches were used as supplementary valida-\ntion methods to investigate the potential of the predicted \ncompounds against the prioritized targets. Both structurally \nclose and diverse analogs can be recognized using similarity \nsearch based on the applied metrics . It is extremely important in the analysis of hits and \nthe schematic representation of Similarity search is shown \nin Fig.\u00a010."}
{"text": "\nDL tools for\u00a0pharmacokinetic property prediction\nPharmacokinetics is the study of how a drug is being \nabsorbed, distributed, metabolized, and excreted from the \nbody. ADMET includes all the pharmacokinetic parameters \nas well as the toxicity profile of the xenobiotic (ADME-\nTox). Hence, the ADME-Tox profile of a lead compound \ncan impact its efficacy and safety. Drug efficacy and safety \nare considered to be some of the major causes of clinical \nfailure of drugs in the process of drug discovery. "}
{"text": "\nThere is an increasing need for an efficient predictive tool of ADMET \nproperties to serve two main purposes\u2014first, at the initial \nstage of selection of new compounds and compound librar-\nies, to reduce the risk of late-stage attrition, and second, to \noptimize the screening and testing by predicting the ADMET \nprofile of the lead compound in drug discovery. In this con-\ntext, ML techniques have been often used in ADME-Tox pre-\ndiction. These predictions are possible due to the availability \nof large amount of pharmacokinetic data of compounds to \nmake models with the help of ADMET in-silico modeling, \nwe can predict several properties such as dose size, dose fre-\nquency, oral absorption, bioavailability, Blood\u2013brain-barrier \n(BBB) . Table\u00a02 lists \nfew examples of ADMET properties that are being modeled \nusing ML algorithms and Table\u00a0 3 denotes some of the ML \nalgorithms used in ADMET predictions."}
{"text": "\nWith the development of different algorithms and \ndetailed analysis over the years, it has been found out that \nthe newly developed DL methodologies when used to predict \nADMET, produce efficient results in comparison to the other \nmentioned model. Researchers all over the world have not \nonly developed specific tools that work on dl techniques for \npredicting one property at a time but also have been able to \nmake complete in silico ADMET platforms. Some of these \ntools and platforms have been mentioned below."}
{"text": "\nTox_(R)CNN\nThe cytotoxic changes induced by drugs cause cellular and \nnuclear morphological changes which are characteristic \nof the specific cell-death pathway involved . These changes are usually being identified based \non the visualization of nuclei using different microscopic \napproaches for years. Tox_(R)CNN is a modern-day tool that \nhas been designed to detect cytotoxicity from microscopic \nimages of fluorescently stained nuclei, without using specific \ntoxicity labeling. The tool Tox_(R)CNN has been operated \nusing DL technologies, as DL is the most powerful super -\nvised ML methodology available and has the exceptional \nabilities to solve computer vision tasks . "}
{"text": "\nTox_(R)CNN\nThis tool uses two convolutional neural net-\nworks (CNN):\n1. Tox_CNN\u2014classifies cells based on prior cell segmen-\ntation and cropping of nuclei images.\n2. Tox_RCNN\u2014carries out fully automated cell identifica-\ntion and classification.\nThese networks provide classification outputs that give \nsensitive screening readouts that detect pre-lethal toxicity \nand hence make the tool extremely useful and affordable \nand even applicable to other in\u00a0vitro toxicity readouts. The \nmodel constitutes a robust screening tool for drug discovery."}
{"text": "\nMetabolite prediction\nStudies show that more than about 25% of the compounds \nare withdrawn from the market or trials due to metabolic, \npharmacokinetic, or toxic problems and hence causes a lot \nof financial loss to the company. The experimental methods \ndeveloped to identify and study the metabolic processes \nin drugs are extremely demanding in terms of equipment, \nexpertise, cost, and time and hence researchers are trying \nto find computational alternatives for the same. The two \nmain research directions which provide necessary support \nand guidance are metabolic sites (SOMs) and metabolite \nstructure that show extensive help in computer-aided meta-\nbolic prediction methods. "}
{"text": "\nMetabolite prediction\nThe model mentioned here performs the function of metabolites prediction in the following \nsteps: it first establishes the database with broad coverage of \nSMARTS-coded metabolic reaction rules. Then, to construct \na DL algorithm-based classification model, the molecular \nfingerprints of compounds are extracted. The model can \nidentify the reaction types that have a higher probability to \noccur in comparison to others. According to the researchers, \nthis method is capable of generating higher accuracies than \nrandom guess and the rule-based methods used for metabo-\nlite prediction ."}
{"text": "\nOral bioavailability prediction\nOral bioavailability plays an important role in determining \nthe absorption of a drug in the body. If we are capable of \npredicting bioavailability, which is difficult to do as it is \ndependent on highly complex factors and processes, then \nit would be extremely easy to prioritize drug candidates in \nthe process of drug discovery. This model uses DL and six \nexperimentally determined in\u00a0vitro and physicochemical \nendpoints including membrane permeation, free fraction, \nmetabolic stability, solubility, pK a value, and lipophilicity \nto determine the oral bioavailability in rats. "}
{"text": "\nOral bioavailability prediction\nThe chemical structure of the drugs is encoded as fingerprints or SMILES. \nModeling the available information along with the structural \ninformation of the chemical compounds, the model achieves \nan accuracy of 70%. It might seem that ADMET predicting \nmodels or tools should be able to predict direct distribu-\ntion, absorptions, etc. of drugs directly, but the fact that sev-\neral parameters govern such pharmacokinetic property and \nprediction of one such parameter is itself highly significant \nand serves as an ADMET tool."}
{"text": "\nOpenChem\nOpenchem is a DL toolkit with Pytorch backend which is \nfreely available via the GitHub repository. It aims at making \neasy-to-use DL models, for computational chemistry and \ndrug design research . This toolkit \nis very helpful in keeping track of the training set, as well \nas in visualizing the evaluations, and project embedding in \nlower dimensional space. The toolkit helps in data preproc-\nessing and has a very fast training process due to the support \nof multi-GPU. It is user-friendly as new models are built \nwith only configuration files. The above toolkit helps us in \nthe classification of data, regression analysis of data, and \nhelps in generating various models which help in predicting \nthe ADME properties of a lead compound. The workflow \ndiagram of OpenChem is shown in Fig.\u00a011."}
{"text": "\nOver the past 2\u00a0decades, many in silico fragment-based \ndrug discovery (FBDD) platform has been developed with \na goal to generate a variety of models to study the phar -\nmacokinetics and physicochemical for drug discovery and \ndevelopment at its early stage. The main principle for the \nfragment-based drug discovery includes a screening of low \nmolecular weight compounds against macromolecular tar -\ngets such as proteins. The screening technology includes dif-\nferential scanning fluorimetry, surface plasmon resonance, \nand thermophoresis, followed by structural characterization \nusing nuclear magnetic resonance (NMR) or X-ray crystal-\nlography of the individually soaked fragments of the lead \ncompounds."}
{"text": "\nTools for\u00a0drug activity prediction\nDL tools are able to identify chemical features in drug mol-\necules and predict the activity of known molecules includ-\ning their biological activity. One such DL-based tool is dis-\ncussed below."}
{"text": "\n \nMultiCon\nTo diminish the workload of a large amount of supervised \ndata and improve the model efficacy, semi-supervised learn-\ning algorithms can be used. By predicting the therapeutic \napplication of drugs from their structural formulas using the \nsemi-supervised learning algorithm, the cost, as well as the \ntime consumed, will reduce significantly. Using the Multi-\nCon toolkit , 1 can classify a drug into \n12 categories, depending on their therapeutic applications \nand image analysis of their structural formulas. "}
{"text": "\nMultiCon\nStudies show that MultiCon has a higher rate of class prediction compared \nto other pre-existing semi-supervised learning algorithms, \nbecause of its rational usage of data balancing, the limited \nnumber of labeled data, online argumentations of drug \nimage input during training, along with the combined usage \nof multi contrastive loss with consistent regularization. The \nworking principle of MultiCon is overviewed in Fig.\u00a012."}
{"text": "\nApplication of\u00a0DL in\u00a0clinical trial design\nThe clinical study is the next step of the drug development \nprocess, which constitute a multi-billion-dollar industry, \naiming for a successful evaluation of drug effectiveness by \ntesting the lead molecules on human subjects . A clinical researcher\u2019s first and foremost objective is \nto figure out that whether the new treatment, like a new drug \nor dosage form or medical device (like a pacemaker) is safe \nand effective for humans to use. Usually, it is designed in a \nway to understand if the newer treatment under investigation \nis more efficacious and has less harmful side effects than the \nconventional treatment available and also design the dosage \nin such a manner based on these results to attain optimum \ntherapeutic effect where the minimal amount of the drug \nis needed to elicit a therapeutic response ."}
{"text": "\nOn an average, it takes 10\u201315\u00a0years with a cost of 1.5\u20132.0 \nbillion USD for new drug molecule to reach the market. \nStudies have shown that approximately half of the time, \nhuman resource and expenditure during the drug discovery \nprocess is spend in the clinical trial phases. The remain-\ning 50% of the time and money covers the preclinical lead \ncompound identification, optimization as well as regulatory \nprocesses . "}
{"text": "\nRecords have shown that \none of the main obstacles in the drug development cycle \nis the high failure rate of clinical trials. The factors lead-\ning to the high failure rates of the clinical trials are patient \ncohort selection and recruitment of mechanisms that fail to \nprovide the best suited patients to a trial in time, as well as \nlack of technical infrastructure to cope with the complexity \nin the later phases of clinical trials, such as the absence of \na reliable and efficient adherence control, patient monitor -\ning, and clinical endpoint detection systems. Past research \nhas witnessed the high potential of AI and other advanced \nanalytics tools to automate clinical trial processes, which \nmakes it a cost-effective approach . Some of \nthe currently available AI-based tools used in clinical trial \nsetup is discusses below."}
{"text": "\nTRIALS.AI\nTrials.ai is an AI-based startup which helps in design of \nclinical trial protocols. It makes use of Natural language \nprocessing and other AI techniques. The software collects \nand analyze data from different source like journals, drug \nlabels, private data from hospitals with which the company \nhave connections. These data are used to study proposed \ntrials, strictness of eligibility criteria, and how it affects the \nclinical trials outcome like cost, participation retention etc. \n."}
{"text": "\nAICURE\nSuccess of well-designed protocols depends on whether the \nparticipants follow the instructions or not. A small mistake, \nlike forgetting to take a pill can negatively impact the study \nresults. AICURE is an AI-based platform which helps clini-\ncal trials participants use their smartphone to record videos \nof them having medication. Using computer vision algo-\nrithms, AICURE software can predict the person have taken \nthe medication or not. This tool can also analyze the facial \nexpression of person to track how they respond to treatment \nwhich can help in development of therapies ."}
{"text": "\nSuccess stories of\u00a0the\u00a0use of\u00a0DL in\u00a0drug discovery \nand\u00a0development\nWith the development of DL methodologies, we have seen \nmajor pharmaceutical companies shifting toward AI, leaving \nbehind the old traditional methods, to maximize patient\u2019s \nprofit along with their own. Astrazeneca is a global, science-\nled multinational pharmaceutical company that have reached \nthe heights of success by integrating AI in every step of drug \ndiscovery going from virtual screening to clinical trials. The \nway they incorporated AI to redefine medical science has \nallowed them to gain a better understanding of existing dis-\neases, identify new targets, design better clinical trials and \nin general speed up the complete process. Astra Zeneca\u2019s \ngrowth is itself a successful example of how integrating AI \nwith medical science can help achieve wonders. Their con-\nstant efforts on increasing AI use can be easily seen by the \ncollaborations they are doing with other AI-based compa-\nnies. One such example is their collaboration with Alibaba \nsubsidiary Ali Health which aims to develop AI-assisted \nscreening and diagnostics platforms in China."}
{"text": "\nThe outbreak of the SARS-CoV-2 virus has brought a \nlarge number of companies under the pressure to find out \nthe best drug in the shortest time possible. To achieve these \ncompanies have resorted to using AI along with the informa-\ntion available. Some examples of such success stories of the \ncompanies who have been able to identify potential leads \nagainst the COVID-19 virus have been mentioned below."}
{"text": "\nMT-DTI (Molecule Transformer Drug Target Interac-\ntion Model), a deep-learning based drug-protein interaction \nprediction model, by Deargen, a South Korea-based com-\npany. This model is used to predict the interaction strength \nbetween a drug and its target protein using simplified chemi-\ncal sequences instead of 2D or 3D molecular structures. The \nmodel was used on the available FDA-approved antiviral \ndrugs, and it found out that Atazanavir, an HIV medication, \nis highly probable to bind and block a prominent protein on \nthe COVID-19 causing virus SARS-CoV-2. "}
{"text": "\nAlong with this, \nit also identified other three antivirals and a not yet approved \ndrug Remdesivir which is now being tested in patients. Dea-\ngen being able to identify antivirals using DL techniques is a \ngreat step on advancing pharmaceutical research and making \nthe process less tedious and more efficient. If such drugs \nare properly tested, there is a great probability, we would be \nable to overcome this pandemic in the shortest time possible \n."}
{"text": "\nAnother example is that of the Benevolent AI, a London \nbased Biotechnology Company that uses biomedical data, \nAI and ML to accelerate research in the health sector. So \nfar, they have been able to identify six drugs and one of \nthem Ruxolitinib is said to be under clinical trial for COVID-\n19 . The company has been using a large \nrepository of medical data, along with information extracted \nfrom the scientific literature by ML, and their AI system \nto identify potential drugs that might block the viral rep-\nlication process of SARS-CoV-2. They got the Food and \nDrug Administration (FDA) approval for the use of their \nproposed Baricitinib drug in combination with Remdesivir \nwhere recovery rate increased for the hospitalized COVID-\n19 patients ."}
{"text": "\nOne of the most common types of cancers across the \nworld is skin cancer. As its incidence is continuously \nincreasing and it is becoming extremely important to detect \nskin cancer in its early stages as studies show early detec -\ntion and treatment is the key to increase the survival rate of \nskin cancer patients. With increasing growth in both medi-\ncal science and AI, a no. of Skin cancer smartphone apps \nhas been launched into the market which provides people \nwith a technological approach who have suspicious lesions \nto decide whether they should seek medical help. "}
{"text": "\nStudies show that about 235 dermatology smartphone apps were \ndeveloped between the years 2014 and 2017 . Earlier, they operated by sending the photo of the \nlesion to a health professional, but now with inbuilt AI algo-\nrithms in smartphones, these apps are successfully capable \nof detecting and classifying images of lesions into high or \nlow risk along with immediate risk assessment and subse-\nquent recommendations to the patient. One such successful \napp is SkinVison ."}
{"text": "\nConclusion\nML has been adapted in the field of drug discovery and \ndevelopment since 1990s with a long and fruitful history \nof DL, the current extension of ML was established based \non the concept of multi-layer neural networks, and some of \nthese techniques have been engrossed only recently in the \npharmaceutical research arena, accelerating the drug dis-\ncovery process. Unmistakably, application of DL needs a \ncautious analysis and a rigorous explanation of their respec-\ntive realm of application that it may provide for the chemists \nin their quest for new drug discovery. "}
{"text": "\nOverall, it is a well-\nappreciated method in the field of modern drug discovery \nand development and with the recent advancements in the \nDL tool box, it can yield far reaching results. Taking into \nconsideration of the recent success of such methodologies \nand its utilization by the pharmaceutical companies to boost \nits search for new drugs, it is convincing that the modern \nDL methodologies will be highly appreciated in the com-\ning era of big data search and analysis for drug design and \ndiscovery."}
{"text": " \nDrug discovery and development has always been \na challenging process which involves a lot of time and \ninvestment to move a drug candidate into the clinical tri-\nals. AI has been proven to demonstrate a substantial pro-\ngress in boosting the success rate of the process, thereby \nreducing the overall costs of the research . Several compa-\nnies have been investing time and money to develop their \nproprietary algorithms such as to tackle the challenges of \nthe modern drug discovery and development process. The \nmajor key to the development of these algorithms is to \nbring together a group of experts from different domains \nsuch as data science engineers, chemists, and biologists \nunder the one ecosystem of in silico drug design ."}
{"text": "\nLack of valid information regarding biological systems \nhave made it difficult to label proper descriptors and end-\npoints, such that AI or DL tools cannot be applied in an \neffective manner to expect a reproducible outcome. Hence, \nthe inability to properly model the biological system is \none of the main limitations of AI in drug design . Another important limitation of AI in drug dis-\ncovery is the selection of appropriate data sets and over \nfocus on speed and cost-effectiveness in drug discovery. \nThe enormous amount of available proxy chemical data \nset and advanced computational techniques, combinato-\nrial chemistry, etc., have made it possible to synthesize \nmore and more molecules in very limited time and that \ntoo in a cost-effective manner, but despite the efforts to \nsynthesizing more molecules, the actual number of drugs \nmaking into the market via clinical trials is still very less \nconsidering the efforts done. "}
{"text": "\nThis is mainly because all \nthe available data and endpoints are generated and used in \nligand design and discovery focusing toward the activity \nagainst the targeted disease thus the ligand might have \ngood pharmacodynamics properties but fails to attain opti-\nmal pharmacokinetic properties thus the primary focus \nmust be on designing a ligand that displays optimum phar -\nmacokinetic and pharmacodynamics properties to become \na good clinical candidate. Rather than focusing on creat-\ning more ligands, we have to come up with novel data, \nand meaningful endpoints which help us to utilize the full \npotential of AI and ML techniques to predict and bring \npotential drug candidates to the clinic which have good \npharmacokinetic properties, safety, and efficacy ."}
{"text": "\nIt is a great opportunity to make use of AI and DL in \nthe field of drug discovery to develop methods and tech-\nniques for proper modeling of biological systems, by gen-\nerating well-validated data having well-defined labels and \nendpoints, providing information regarding the targets and \ntheir interactions with ligands."}
